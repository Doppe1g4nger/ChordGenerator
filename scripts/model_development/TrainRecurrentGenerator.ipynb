{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chord_to_index = {\n",
    "    '000000000000': 48,\n",
    "    '000000100001': 105,\n",
    "    '000001000010': 106,\n",
    "    '000001001001': 70,\n",
    "    '000010000001': 100,\n",
    "    '000010000100': 107,\n",
    "    '000010001001': 1,\n",
    "    '000010010001': 22,\n",
    "    '000010010010': 71,\n",
    "    '000100000010': 101,\n",
    "    '000100001000': 108,\n",
    "    '000100001001': 18,\n",
    "    '000100010010': 2,\n",
    "    '000100011001': 55,\n",
    "    '000100100001': 6,\n",
    "    '000100100010': 23,\n",
    "    '000100100011': 98,\n",
    "    '000100100100': 72,\n",
    "    '000100100101': 37,\n",
    "    '000100101001': 28,\n",
    "    '000101001001': 87,\n",
    "    '000110001001': 93,\n",
    "    '000110010001': 59,\n",
    "    '001000000100': 102,\n",
    "    '001000001001': 67,\n",
    "    '001000010000': 109,\n",
    "    '001000010001': 10,\n",
    "    '001000010010': 19,\n",
    "    '001000100001': 15,\n",
    "    '001000100011': 52,\n",
    "    '001000100100': 3,\n",
    "    '001000100101': 25,\n",
    "    '001000101001': 84,\n",
    "    '001000110001': 90,\n",
    "    '001000110010': 56,\n",
    "    '001001000001': 64,\n",
    "    '001001000010': 7,\n",
    "    '001001000100': 12,\n",
    "    '001001000101': 81,\n",
    "    '001001000110': 99,\n",
    "    '001001001000': 73,\n",
    "    '001001001001': 51,\n",
    "    '001001001010': 38,\n",
    "    '001001010001': 41,\n",
    "    '001001010010': 29,\n",
    "    '001010001001': 44,\n",
    "    '001010010001': 32,\n",
    "    '001010010010': 76,\n",
    "    '001010101101': 113,\n",
    "    '001010110101': 128,\n",
    "    '001100010010': 94,\n",
    "    '001100100010': 60,\n",
    "    '010000001000': 103,\n",
    "    '010000010010': 68,\n",
    "    '010000100000': 110,\n",
    "    '010000100010': 11,\n",
    "    '010000100100': 20,\n",
    "    '010001000010': 16,\n",
    "    '010001000110': 53,\n",
    "    '010001001000': 4,\n",
    "    '010001001001': 47,\n",
    "    '010001001010': 26,\n",
    "    '010001010010': 85,\n",
    "    '010001100010': 91,\n",
    "    '010001100100': 57,\n",
    "    '010010000010': 65,\n",
    "    '010010000100': 8,\n",
    "    '010010001000': 13,\n",
    "    '010010001001': 35,\n",
    "    '010010001010': 82,\n",
    "    '010010001100': 88,\n",
    "    '010010010000': 74,\n",
    "    '010010010001': 79,\n",
    "    '010010010010': 49,\n",
    "    '010010010100': 39,\n",
    "    '010010100010': 42,\n",
    "    '010010100100': 30,\n",
    "    '010010101011': 123,\n",
    "    '010010101101': 126,\n",
    "    '010100010010': 45,\n",
    "    '010100100010': 33,\n",
    "    '010100100100': 77,\n",
    "    '010100101011': 124,\n",
    "    '010101011010': 114,\n",
    "    '010101101001': 116,\n",
    "    '010101101010': 129,\n",
    "    '010110100101': 118,\n",
    "    '010110101001': 131,\n",
    "    '011000100100': 95,\n",
    "    '011001000100': 61,\n",
    "    '011010010101': 120,\n",
    "    '011010100101': 133,\n",
    "    '100000010000': 104,\n",
    "    '100000100100': 69,\n",
    "    '100001000000': 111,\n",
    "    '100001000100': 0,\n",
    "    '100001001000': 21,\n",
    "    '100010000100': 17,\n",
    "    '100010001100': 54,\n",
    "    '100010010000': 5,\n",
    "    '100010010001': 97,\n",
    "    '100010010010': 36,\n",
    "    '100010010100': 27,\n",
    "    '100010100100': 86,\n",
    "    '100011000100': 92,\n",
    "    '100011001000': 58,\n",
    "    '100100000100': 66,\n",
    "    '100100001000': 9,\n",
    "    '100100010000': 14,\n",
    "    '100100010001': 63,\n",
    "    '100100010010': 24,\n",
    "    '100100010100': 83,\n",
    "    '100100011000': 89,\n",
    "    '100100100000': 75,\n",
    "    '100100100010': 80,\n",
    "    '100100100100': 50,\n",
    "    '100100101000': 40,\n",
    "    '100101000100': 43,\n",
    "    '100101001000': 31,\n",
    "    '100101010110': 112,\n",
    "    '100101011010': 127,\n",
    "    '101000100100': 46,\n",
    "    '101001000100': 34,\n",
    "    '101001001000': 78,\n",
    "    '101001010101': 122,\n",
    "    '101001010110': 125,\n",
    "    '101010010101': 135,\n",
    "    '101010110100': 115,\n",
    "    '101011010010': 117,\n",
    "    '101011010100': 130,\n",
    "    '101101001010': 119,\n",
    "    '101101010010': 132,\n",
    "    '110001001000': 96,\n",
    "    '110010001000': 62,\n",
    "    '110100101010': 121,\n",
    "    '110101001010': 134\n",
    "}\n",
    "\n",
    "index_to_chord = {\n",
    "    0: '100001000100',\n",
    "    1: '000010001001',\n",
    "    2: '000100010010',\n",
    "    3: '001000100100',\n",
    "    4: '010001001000',\n",
    "    5: '100010010000',\n",
    "    6: '000100100001',\n",
    "    7: '001001000010',\n",
    "    8: '010010000100',\n",
    "    9: '100100001000',\n",
    "    10: '001000010001',\n",
    "    11: '010000100010',\n",
    "    12: '001001000100',\n",
    "    13: '010010001000',\n",
    "    14: '100100010000',\n",
    "    15: '001000100001',\n",
    "    16: '010001000010',\n",
    "    17: '100010000100',\n",
    "    18: '000100001001',\n",
    "    19: '001000010010',\n",
    "    20: '010000100100',\n",
    "    21: '100001001000',\n",
    "    22: '000010010001',\n",
    "    23: '000100100010',\n",
    "    24: '100100010010',\n",
    "    25: '001000100101',\n",
    "    26: '010001001010',\n",
    "    27: '100010010100',\n",
    "    28: '000100101001',\n",
    "    29: '001001010010',\n",
    "    30: '010010100100',\n",
    "    31: '100101001000',\n",
    "    32: '001010010001',\n",
    "    33: '010100100010',\n",
    "    34: '101001000100',\n",
    "    35: '010010001001',\n",
    "    36: '100010010010',\n",
    "    37: '000100100101',\n",
    "    38: '001001001010',\n",
    "    39: '010010010100',\n",
    "    40: '100100101000',\n",
    "    41: '001001010001',\n",
    "    42: '010010100010',\n",
    "    43: '100101000100',\n",
    "    44: '001010001001',\n",
    "    45: '010100010010',\n",
    "    46: '101000100100',\n",
    "    47: '010001001001',\n",
    "    48: '000000000000',\n",
    "    49: '010010010010',\n",
    "    50: '100100100100',\n",
    "    51: '001001001001',\n",
    "    52: '001000100011',\n",
    "    53: '010001000110',\n",
    "    54: '100010001100',\n",
    "    55: '000100011001',\n",
    "    56: '001000110010',\n",
    "    57: '010001100100',\n",
    "    58: '100011001000',\n",
    "    59: '000110010001',\n",
    "    60: '001100100010',\n",
    "    61: '011001000100',\n",
    "    62: '110010001000',\n",
    "    63: '100100010001',\n",
    "    64: '001001000001',\n",
    "    65: '010010000010',\n",
    "    66: '100100000100',\n",
    "    67: '001000001001',\n",
    "    68: '010000010010',\n",
    "    69: '100000100100',\n",
    "    70: '000001001001',\n",
    "    71: '000010010010',\n",
    "    72: '000100100100',\n",
    "    73: '001001001000',\n",
    "    74: '010010010000',\n",
    "    75: '100100100000',\n",
    "    76: '001010010010',\n",
    "    77: '010100100100',\n",
    "    78: '101001001000',\n",
    "    79: '010010010001',\n",
    "    80: '100100100010',\n",
    "    81: '001001000101',\n",
    "    82: '010010001010',\n",
    "    83: '100100010100',\n",
    "    84: '001000101001',\n",
    "    85: '010001010010',\n",
    "    86: '100010100100',\n",
    "    87: '000101001001',\n",
    "    88: '010010001100',\n",
    "    89: '100100011000',\n",
    "    90: '001000110001',\n",
    "    91: '010001100010',\n",
    "    92: '100011000100',\n",
    "    93: '000110001001',\n",
    "    94: '001100010010',\n",
    "    95: '011000100100',\n",
    "    96: '110001001000',\n",
    "    97: '100010010001',\n",
    "    98: '000100100011',\n",
    "    99: '001001000110',\n",
    "    100: '000010000001',\n",
    "    101: '000100000010',\n",
    "    102: '001000000100',\n",
    "    103: '010000001000',\n",
    "    104: '100000010000',\n",
    "    105: '000000100001',\n",
    "    106: '000001000010',\n",
    "    107: '000010000100',\n",
    "    108: '000100001000',\n",
    "    109: '001000010000',\n",
    "    110: '010000100000',\n",
    "    111: '100001000000',\n",
    "    112: '100101010110',\n",
    "    113: '001010101101',\n",
    "    114: '010101011010',\n",
    "    115: '101010110100',\n",
    "    116: '010101101001',\n",
    "    117: '101011010010',\n",
    "    118: '010110100101',\n",
    "    119: '101101001010',\n",
    "    120: '011010010101',\n",
    "    121: '110100101010',\n",
    "    122: '101001010101',\n",
    "    123: '010010101011',\n",
    "    124: '010100101011',\n",
    "    125: '101001010110',\n",
    "    126: '010010101101',\n",
    "    127: '100101011010',\n",
    "    128: '001010110101',\n",
    "    129: '010101101010',\n",
    "    130: '101011010100',\n",
    "    131: '010110101001',\n",
    "    132: '101101010010',\n",
    "    133: '011010100101',\n",
    "    134: '110101001010',\n",
    "    135: '101010010101'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelodyToChordSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This dataset loads the training data linking a 128 * 13 (optionally 14 with start bit)\n",
    "    melody index input to a 128 * 12 multi-hot or 128 * 136 one-hot chord sequence output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, seq_length=128, batch_size=24,\n",
    "                 transpose=None, strip_rest_edge_measures=False, \n",
    "                 include_start_bit=False, chords_as_one_hot=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (string):\n",
    "                directory where .txt files of training data are stored\n",
    "            transpose (list[int]):\n",
    "                list of semitones to transpose the data by\n",
    "            strip_rest_edge_measures:\n",
    "                whether or not to skip measures before the first and after the last melody note\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_dir = data_dir\n",
    "        self.transpose = transpose\n",
    "        self.strip_rest_edge_measures = strip_rest_edge_measures\n",
    "        self.include_start_bit = include_start_bit\n",
    "#         self.chords_as_one_hot = chords_as_one_hot\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "#         self.number_of_chords = 0\n",
    "#         if self.chords_as_one_hot:\n",
    "#             self.chord_to_index_map = {}\n",
    "#             self.index_to_chord_map = {}\n",
    "        \n",
    "        self._song_tuples = []\n",
    "        \n",
    "        for file in os.listdir(data_dir):\n",
    "            with open(os.path.join(data_dir, file)) as f_in:\n",
    "                file_text = f_in.read()\n",
    "                \n",
    "            lines = file_text.split(\"\\n\")\n",
    "            lines = [line for line in lines if line != \"\"]\n",
    "            \n",
    "            song_has_melody = False\n",
    "            for line in lines:\n",
    "                if line.split()[0] != \"12\":\n",
    "                    song_has_melody = True\n",
    "                    break\n",
    "                        \n",
    "            if song_has_melody:\n",
    "#                 if strip_rest_edge_measures:\n",
    "#                     while lines[0].split()[0] == \"12\":\n",
    "#                         lines.pop(0)\n",
    "\n",
    "#                     while lines[-1].split()[0] == \"12\":\n",
    "#                         lines.pop(-1)\n",
    "                    \n",
    "                song_melody = []\n",
    "                song_harmony = []\n",
    "\n",
    "                melody_quintet = []\n",
    "                for i, line in enumerate(lines):\n",
    "                    melody_index, harmony_vector = line.split()\n",
    "                    melody_index = int(melody_index)\n",
    "                    melody_step_vector = [1 if j == melody_index else 0 for j in range(13)]\n",
    "                    if include_start_bit:\n",
    "                        melody_step_vector.append(1 if i in range(16) else 0)\n",
    "                        \n",
    "                    melody_quintet.append(melody_step_vector)\n",
    "                    \n",
    "                    if i % 4 == 3:\n",
    "                        song_melody.append(melody_quintet)\n",
    "                        song_harmony.append([int(val) for val in harmony_vector])\n",
    "                        melody_quintet = []\n",
    "                    \n",
    "#                     if self.chords_as_one_hot:\n",
    "#                         harmony_multihot = \"\".join([str(int(val)) for val in harmony_vector])\n",
    "#                         if harmony_multihot not in self.chord_to_index_map.keys():\n",
    "#                             self.chord_to_index_map[harmony_multihot] = self.number_of_chords\n",
    "#                             self.index_to_chord_map[self.number_of_chords] = harmony_multihot\n",
    "#                             self.number_of_chords += 1\n",
    "#                             if transpose:\n",
    "#                                 for semitone_amount in transpose:\n",
    "#                                     shifted_harmony_multihot = []\n",
    "#                                     for j in range(12):\n",
    "#                                         shifted_harmony_multihot.append(harmony_multihot[(j + semitone_amount) % 12])\n",
    "#                                     shifted_harmony_multihot = \"\".join(shifted_harmony_multihot)\n",
    "#                                     if shifted_harmony_multihot not in self.chord_to_index_map.keys():\n",
    "#                                         self.chord_to_index_map[shifted_harmony_multihot] = self.number_of_chords\n",
    "#                                         self.index_to_chord_map[self.number_of_chords] = shifted_harmony_multihot\n",
    "#                                         self.number_of_chords += 1\n",
    "                    \n",
    "                song_melody = torch.tensor(song_melody, dtype=torch.uint8, device=device)\n",
    "                song_harmony = torch.tensor(song_harmony, dtype=torch.uint8, device=device)\n",
    "\n",
    "                self._song_tuples.append([song_melody, song_harmony])\n",
    "                    \n",
    "                if transpose:\n",
    "                    for semitone_amount in transpose:\n",
    "                        shifted_melody = copy.deepcopy(song_melody)\n",
    "                        shifted_melody[:,:,:12] = shifted_melody[:,:,:12].roll(semitone_amount, dims=2)\n",
    "                        shifted_harmony = copy.deepcopy(song_harmony).roll(semitone_amount, dims=1)\n",
    "\n",
    "                        self._song_tuples.append([shifted_melody, shifted_harmony])\n",
    "                    \n",
    "            else:\n",
    "                print(\"Skipping entry with no melody\")\n",
    "                \n",
    "#         if self.chords_as_one_hot:\n",
    "#             for i in range(len(self._song_tuples)):\n",
    "#                 if i % 100 == 0:\n",
    "#                     print(i)\n",
    "#                 song_one_hot = []\n",
    "#                 har_vectors = self._song_tuples[i][1]\n",
    "#                 for har_vec in har_vectors:\n",
    "#                     har_index = self.chord_to_index_map[\"\".join([str(int(val)) for val in har_vec])]\n",
    "#                     song_one_hot.append(har_index)\n",
    "                    \n",
    "#                 self._song_tuples[i][1] = torch.tensor(song_one_hot, dtype=torch.uint8, device=device)\n",
    "                \n",
    "        print(len(self._song_tuples))\n",
    "        print(self._song_tuples[0][0].shape)\n",
    "        print(self._song_tuples[0][1].shape)\n",
    "            \n",
    "        random.shuffle(self._song_tuples)\n",
    "\n",
    "#         self.validation_set = self._song_tuples[int(1 * len(self._song_tuples)):]\n",
    "        self._song_tuples = self._song_tuples[:int(1 * len(self._song_tuples))]\n",
    "        \n",
    "#         self.melody_sequence_val = torch.tensor([], dtype=torch.uint8, device=device)\n",
    "#         self.harmony_sequence_val = torch.tensor([], dtype=torch.uint8, device=device)\n",
    "        \n",
    "#         for mel, har in self.validation_set:\n",
    "#             self.melody_sequence_val = torch.cat((self.melody_sequence_val, mel))\n",
    "#             self.harmony_sequence_val = torch.cat((self.harmony_sequence_val, har))\n",
    "            \n",
    "#         print(self.melody_sequence_val.shape)\n",
    "#         print(self.harmony_sequence_val.shape)\n",
    "\n",
    "#         self.melody_sequence_val = self.melody_sequence_val.view(1, -1)\n",
    "#         self.harmony_sequence_val = self.harmony_sequence_val.view(1, -1)\n",
    "        \n",
    "#         print(self.melody_sequence_val.shape)\n",
    "#         print(self.harmony_sequence_val.shape)\n",
    "        \n",
    "#         self.melody_sequence_val = self.melody_sequence_val.view(\n",
    "#             self.batch_size, -1, 14 if self.include_start_bit else 13\n",
    "#         )\n",
    "#         self.harmony_sequence_val = self.harmony_sequence_val.view(\n",
    "#             self.batch_size, -1, 1 if self.chords_as_one_hot else 12\n",
    "#         )\n",
    "        \n",
    "#         print(self.melody_sequence_val.shape)\n",
    "#         print(self.harmony_sequence_val.shape)\n",
    "        \n",
    "        self.melody_sequence = torch.tensor([], dtype=torch.uint8, device=device)\n",
    "        self.harmony_sequence = torch.tensor([], dtype=torch.uint8, device=device)\n",
    "                \n",
    "        for mel, har in self._song_tuples:\n",
    "            self.melody_sequence = torch.cat((self.melody_sequence, torch.flatten(mel, start_dim=1, end_dim=2)))\n",
    "            self.harmony_sequence = torch.cat((self.harmony_sequence, har))\n",
    "            \n",
    "        print(self.melody_sequence.shape)\n",
    "        print(self.harmony_sequence.shape)\n",
    "        \n",
    "\n",
    "        self.melody_sequence = self.melody_sequence.view(\n",
    "            self.batch_size, -1, 52\n",
    "        )\n",
    "        self.harmony_sequence = self.harmony_sequence.view(\n",
    "            self.batch_size, -1, 12\n",
    "        )\n",
    "        \n",
    "        print(self.melody_sequence.shape)\n",
    "        print(self.harmony_sequence.shape)\n",
    "        \n",
    "        \n",
    "    def reshuffle_sequence(self):\n",
    "        self.melody_sequence = torch.tensor([], dtype=torch.uint8, device=device)\n",
    "        self.harmony_sequence = torch.tensor([], dtype=torch.uint8, device=device)\n",
    "        \n",
    "        random.shuffle(self._song_tuples)\n",
    "        \n",
    "        for mel, har in self._song_tuples:\n",
    "            self.melody_sequence = torch.cat((self.melody_sequence, mel))\n",
    "            self.harmony_sequence = torch.cat((self.harmony_sequence, har))\n",
    "\n",
    "        self.melody_sequence = self.melody_sequence.view(\n",
    "            self.batch_size, -1, 52\n",
    "        )\n",
    "        self.harmony_sequence = self.harmony_sequence.view(\n",
    "            self.batch_size, -1, 12\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return ceil(self.melody_sequence.shape[1] / self.seq_length)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        melody, harmony = (self.melody_sequence[:, idx*self.seq_length:(idx+1)*self.seq_length],\n",
    "                           self.harmony_sequence[:, idx*self.seq_length:(idx+1)*self.seq_length])\n",
    "        \n",
    "        return {\"melody\": melody, \"harmony\": harmony}\n",
    "    \n",
    "    \n",
    "    def get_validation_set(self, idx):\n",
    "        melody, harmony = (self.melody_sequence_val[:, idx*self.seq_length:(idx+1)*self.seq_length],\n",
    "                           self.harmony_sequence_val[:, idx*self.seq_length:(idx+1)*self.seq_length])\n",
    "        \n",
    "        return {\"melody\": melody, \"harmony\": harmony}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input the directory where training files are stored: ../../data/training_data/training_dt_longnotes\n"
     ]
    }
   ],
   "source": [
    "training_data_fp = input(\"Input the directory where training files are stored: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input the name of the file you would like to store the model in: ../../data/models/model10.pt\n"
     ]
    }
   ],
   "source": [
    "model_path = input(\"Input the name of the file you would like to store the model in: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entry with no melody\n",
      "Skipping entry with no melody\n",
      "Skipping entry with no melody\n",
      "Skipping entry with no melody\n",
      "Skipping entry with no melody\n",
      "Skipping entry with no melody\n",
      "2328\n",
      "torch.Size([732, 4, 13])\n",
      "torch.Size([732, 12])\n",
      "torch.Size([989136, 52])\n",
      "torch.Size([989136, 12])\n",
      "torch.Size([24, 41214, 52])\n",
      "torch.Size([24, 41214, 12])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MelodyToChordSequenceDataset(\n",
    "    training_data_fp,\n",
    "    batch_size=24,\n",
    "    transpose=[i for i in range(1, 12)],\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset)\n",
    "\n",
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.9463, 2.9463, 2.9463, 2.9463, 2.9463, 2.9463, 2.9463, 2.9463, 2.9463,\n",
      "        2.9463, 2.9463, 2.9463], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "positive_examples = torch.zeros([12], device=device)\n",
    "\n",
    "for datum in dataloader:\n",
    "    datum[\"harmony\"].squeeze_()\n",
    "    data_batch = datum[\"harmony\"].to(torch.float).sum(1).sum(0)\n",
    "    positive_examples += data_batch\n",
    "\n",
    "negative_examples = (len(dataloader) * 24 * 128) - positive_examples\n",
    "\n",
    "positive_weights = torch.div(negative_examples, positive_examples).flatten()\n",
    "\n",
    "print(positive_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define recurrent prediction model\n",
    "# class LSTMGenerator(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(LSTMGenerator, self).__init__()\n",
    "#         self.embedding = nn.Embedding(13, 100)\n",
    "#         self.lstm = nn.LSTM(input_size=100, hidden_size=256, num_layers=2, batch_first=True)\n",
    "#         self.fc1 = nn.Linear(256, 12)\n",
    "        \n",
    "#     def forward(self, x, hidden_in):\n",
    "#         x = self.embedding(x)\n",
    "#         x = x.view(48, -1, 100)\n",
    "#         x, h_out = self.lstm(x, hidden_in)\n",
    "#         x = self.fc1(x)\n",
    "#         return x, h_out\n",
    "\n",
    "# net = LSTMGenerator()\n",
    "\n",
    "# net.to(device)\n",
    "# print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define recurrent prediction model\n",
    "# # NOTE: TOOK OUT EMBEDDING LAYER, MODEL 5 wont work with this spec\n",
    "# class LSTMGenerator_v2(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(LSTMGenerator_v2, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size=13, \n",
    "#                             hidden_size=256, \n",
    "#                             num_layers=2, \n",
    "#                             batch_first=True, \n",
    "#                             bidirectional=True, \n",
    "#                             dropout=0.2)\n",
    "#         self.fc1 = nn.Linear(512, 256)\n",
    "#         self.fc2 = nn.Linear(256, 12)\n",
    "        \n",
    "#     def forward(self, x, hidden_in):\n",
    "#         x, h_out = self.lstm(x, hidden_in)\n",
    "#         x = self.fc1(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "        \n",
    "#         return x, h_out\n",
    "\n",
    "# net = LSTMGenerator_v2()\n",
    "\n",
    "# net.to(device)\n",
    "# print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define recurrent prediction model\n",
    "# class LSTMGenerator_v3(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(LSTMGenerator_v3, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size=14, \n",
    "#                             hidden_size=256, \n",
    "#                             num_layers=2, \n",
    "#                             batch_first=True, \n",
    "#                             bidirectional=True, \n",
    "#                             dropout=0.2)\n",
    "#         self.fc1 = nn.Linear(512, 256)\n",
    "#         self.fc2 = nn.Linear(256, 12)\n",
    "        \n",
    "#     def forward(self, x, hidden_in):\n",
    "#         x, h_out = self.lstm(x, hidden_in)\n",
    "#         x = self.fc1(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "        \n",
    "#         return x, h_out\n",
    "\n",
    "# net = LSTMGenerator_v3()\n",
    "\n",
    "# net.to(device)\n",
    "# print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define recurrent prediction model\n",
    "# class LSTMGenerator_v4(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(LSTMGenerator_v4, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size=13, \n",
    "#                             hidden_size=256, \n",
    "#                             num_layers=2, \n",
    "#                             batch_first=True, \n",
    "#                             bidirectional=True, \n",
    "#                             dropout=0.2)\n",
    "#         self.fc1 = nn.Linear(512, 256)\n",
    "#         self.fc2 = nn.Linear(256, 136)\n",
    "#         self.ls = nn.LogSoftmax()\n",
    "        \n",
    "#     def forward(self, x, hidden_in):\n",
    "#         x, h_out = self.lstm(x, hidden_in)\n",
    "#         x = self.fc1(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.ls(x)\n",
    "        \n",
    "#         return x, h_out\n",
    "\n",
    "# net = LSTMGenerator_v4()\n",
    "\n",
    "# net.to(device)\n",
    "# print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMGenerator_v5(\n",
      "  (lstm): LSTM(52, 256, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=12, bias=True)\n",
      ")\n",
      "2346252\n"
     ]
    }
   ],
   "source": [
    "# Define recurrent prediction model\n",
    "class LSTMGenerator_v5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMGenerator_v5, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=52, \n",
    "                            hidden_size=256, \n",
    "                            num_layers=2, \n",
    "                            batch_first=True, \n",
    "                            bidirectional=True, \n",
    "                            dropout=0.2)\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.fc2 = nn.Linear(256, 12)\n",
    "        \n",
    "    def forward(self, x, hidden_in):\n",
    "        x, h_out = self.lstm(x, hidden_in)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x, h_out\n",
    "\n",
    "net = LSTMGenerator_v5()\n",
    "\n",
    "net.to(device)\n",
    "print(net)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 0.49870256172574084\n",
      "[2] loss: 0.45334619533571396\n",
      "[3] loss: 0.4418628060299417\n",
      "[4] loss: 0.4360727612276255\n",
      "[5] loss: 0.43282970591731695\n",
      "[6] loss: 0.42766388501069563\n",
      "[7] loss: 0.42559942677154305\n",
      "[8] loss: 0.4211236955771535\n",
      "[9] loss: 0.41881091050479724\n",
      "[10] loss: 0.4131768362492508\n",
      "[11] loss: 0.40871451980208756\n",
      "[12] loss: 0.40399421936606766\n",
      "[13] loss: 0.4004759283169456\n",
      "[14] loss: 0.39550783276928136\n",
      "[15] loss: 0.3895345901295265\n",
      "[16] loss: 0.3837540406062736\n",
      "[17] loss: 0.3781624823253347\n",
      "[18] loss: 0.3696305239978044\n",
      "[19] loss: 0.3621245985445769\n",
      "[20] loss: 0.3520688649660312\n",
      "[21] loss: 0.3425037948796468\n",
      "[22] loss: 0.3323477803735259\n",
      "[23] loss: 0.32226525973644315\n",
      "[24] loss: 0.31058716551857707\n",
      "[25] loss: 0.3016244199427759\n",
      "[26] loss: 0.29083006023805336\n",
      "[27] loss: 0.28205288159921305\n",
      "[28] loss: 0.27314736698725206\n",
      "[29] loss: 0.2664947947545081\n",
      "[30] loss: 0.2577383416409818\n",
      "[31] loss: 0.2518176865411101\n",
      "[32] loss: 0.24571940122368913\n",
      "[33] loss: 0.23909556463083126\n",
      "[34] loss: 0.23435359364896088\n",
      "[35] loss: 0.22980557048913114\n",
      "[36] loss: 0.225619365709909\n",
      "[37] loss: 0.2216542881005299\n",
      "[38] loss: 0.21768735608328943\n",
      "[39] loss: 0.2127103840249666\n",
      "[40] loss: 0.21031544996159418\n",
      "[41] loss: 0.20685395778891463\n",
      "[42] loss: 0.20430457601265878\n",
      "[43] loss: 0.200824003134455\n",
      "[44] loss: 0.1985811683933557\n",
      "[45] loss: 0.19681136874678712\n",
      "[46] loss: 0.1934438259427592\n",
      "[47] loss: 0.19192279329211076\n",
      "[48] loss: 0.18877289272983622\n",
      "[49] loss: 0.18668173665400617\n",
      "[50] loss: 0.18602112377559918\n",
      "[51] loss: 0.18346069137687268\n",
      "[52] loss: 0.18166115842990993\n",
      "[53] loss: 0.1804766820176788\n",
      "[54] loss: 0.17831035870017473\n",
      "[55] loss: 0.17703343684906545\n",
      "[56] loss: 0.17519420912917355\n",
      "[57] loss: 0.17355376056262425\n",
      "[58] loss: 0.17182025692847944\n",
      "[59] loss: 0.17033264866989592\n",
      "[60] loss: 0.1691136504552379\n",
      "[61] loss: 0.16712335804882256\n",
      "[62] loss: 0.16720896785766443\n",
      "[63] loss: 0.1654319009101539\n",
      "[64] loss: 0.16451369910876942\n",
      "[65] loss: 0.16241305821245502\n",
      "[66] loss: 0.1615854111609992\n",
      "[67] loss: 0.16099143791828097\n",
      "[68] loss: 0.16061688346429642\n",
      "[69] loss: 0.15981004440358706\n",
      "[70] loss: 0.15864361732365181\n",
      "[71] loss: 0.1567196401750079\n",
      "[72] loss: 0.15606297899662339\n",
      "[73] loss: 0.15534807357543745\n",
      "[74] loss: 0.15327127807721588\n",
      "[75] loss: 0.15228308323099746\n",
      "[76] loss: 0.1510342335710244\n",
      "[77] loss: 0.15085418994382302\n",
      "[78] loss: 0.14949085433845935\n",
      "[79] loss: 0.14971330699435673\n",
      "[80] loss: 0.14933477159335007\n",
      "[81] loss: 0.14818004673774937\n",
      "[82] loss: 0.1473472351613252\n",
      "[83] loss: 0.14633587569646214\n",
      "[84] loss: 0.14554330475211885\n",
      "[85] loss: 0.14500735304462983\n",
      "[86] loss: 0.1438251110661474\n",
      "[87] loss: 0.1432004637136963\n",
      "[88] loss: 0.14178446886239585\n",
      "[89] loss: 0.14212979913128088\n",
      "[90] loss: 0.14085891107039422\n",
      "[91] loss: 0.14058961082967172\n",
      "[92] loss: 0.13969214084725942\n",
      "[93] loss: 0.13903122381393954\n",
      "[94] loss: 0.13975798822245242\n",
      "[95] loss: 0.13821493068374463\n",
      "[96] loss: 0.13796208125556478\n",
      "[97] loss: 0.13681022748813865\n",
      "[98] loss: 0.1354634468183384\n",
      "[99] loss: 0.13555363061265174\n",
      "[100] loss: 0.1352072974308307\n",
      "[101] loss: 0.1344769746759293\n",
      "[102] loss: 0.13347410922002348\n",
      "[103] loss: 0.1332512401414584\n",
      "[104] loss: 0.13354774913895204\n",
      "[105] loss: 0.13328729192755237\n",
      "[106] loss: 0.13151643138309443\n",
      "[107] loss: 0.13226960955754571\n",
      "[108] loss: 0.13113113302991997\n",
      "[109] loss: 0.1306862285973863\n",
      "[110] loss: 0.1306438270655478\n",
      "[111] loss: 0.12945607061619344\n",
      "[112] loss: 0.12790739603386903\n",
      "[113] loss: 0.1288309169944769\n",
      "[114] loss: 0.1270883645145049\n",
      "[115] loss: 0.12710166495779288\n",
      "[116] loss: 0.12754692910065563\n",
      "[117] loss: 0.12650448185686738\n",
      "[118] loss: 0.12754932073702724\n",
      "[119] loss: 0.12578765471807177\n",
      "[120] loss: 0.1274036436995364\n",
      "[121] loss: 0.1284914630355302\n",
      "[122] loss: 0.12520171158154558\n",
      "[123] loss: 0.12380008971006233\n",
      "[124] loss: 0.1225479277232604\n",
      "[125] loss: 0.12216729977347465\n",
      "[126] loss: 0.12261682405188587\n",
      "[127] loss: 0.12227723462888913\n",
      "[128] loss: 0.1226717921332543\n",
      "[129] loss: 0.12187978889075865\n",
      "[130] loss: 0.12186530100540345\n",
      "[131] loss: 0.12122025868352156\n",
      "[132] loss: 0.12053237867003642\n",
      "[133] loss: 0.12014836431567713\n",
      "[134] loss: 0.11967034065251395\n",
      "[135] loss: 0.11980529076286725\n",
      "[136] loss: 0.11916623785166267\n",
      "[137] loss: 0.11903440554319701\n",
      "[138] loss: 0.11895935392435293\n",
      "[139] loss: 0.11721395430450114\n",
      "[140] loss: 0.11747297498842944\n",
      "[141] loss: 0.11726755609516032\n",
      "[142] loss: 0.11685780806015737\n",
      "[143] loss: 0.11632609027235405\n",
      "[144] loss: 0.11569468621511637\n",
      "[145] loss: 0.1167749054035785\n",
      "[146] loss: 0.1159383693119939\n",
      "[147] loss: 0.11874360163111865\n",
      "[148] loss: 0.12498329859805403\n",
      "[149] loss: 0.11909730905979317\n",
      "[150] loss: 0.11551204960075964\n",
      "[151] loss: 0.11426671201027698\n",
      "[152] loss: 0.114058710819816\n",
      "[153] loss: 0.11318493386880951\n",
      "[154] loss: 0.11326197464227306\n",
      "[155] loss: 0.11441599010773328\n",
      "[156] loss: 0.11364679991828729\n",
      "[157] loss: 0.11279003243453754\n",
      "[158] loss: 0.11148889735341072\n",
      "[159] loss: 0.11187982503671824\n",
      "[160] loss: 0.11164262563406681\n",
      "[161] loss: 0.11052399820515088\n",
      "[162] loss: 0.11111370397743231\n",
      "[163] loss: 0.10986431942593237\n",
      "[164] loss: 0.11097987364075199\n",
      "[165] loss: 0.1100147797245972\n",
      "[166] loss: 0.10967309873620545\n",
      "[167] loss: 0.11049078374823428\n",
      "[168] loss: 0.11010712600365188\n",
      "[169] loss: 0.10990083635269855\n",
      "[170] loss: 0.10851838735875136\n",
      "[171] loss: 0.10833885757865742\n",
      "[172] loss: 0.1092314318933102\n",
      "[173] loss: 0.10774108315152782\n",
      "[174] loss: 0.1077798338988738\n",
      "[175] loss: 0.1085235821488111\n",
      "[176] loss: 0.10741098830187172\n",
      "[177] loss: 0.107651291449803\n",
      "[178] loss: 0.10878919111275524\n",
      "[179] loss: 0.10740816484419455\n",
      "[180] loss: 0.10613545634268974\n",
      "[181] loss: 0.10745759604510313\n",
      "[182] loss: 0.10714992112624719\n",
      "[183] loss: 0.1071711552231023\n",
      "[184] loss: 0.1075143116462675\n",
      "[185] loss: 0.10711009038022216\n",
      "[186] loss: 0.1057941038045824\n",
      "[187] loss: 0.10594119791010892\n",
      "[188] loss: 0.10506748498319099\n",
      "[189] loss: 0.10465993402491075\n",
      "[190] loss: 0.10922122106927892\n",
      "[191] loss: 0.10600050165092353\n",
      "[192] loss: 0.10408251370471086\n",
      "[193] loss: 0.1033637507107132\n",
      "[194] loss: 0.10337117330535599\n",
      "[195] loss: 0.10288070024263045\n",
      "[196] loss: 0.10308043448173482\n",
      "[197] loss: 0.1037122666118493\n",
      "[198] loss: 0.10406870281566745\n",
      "[199] loss: 0.10418953918938681\n",
      "[200] loss: 0.10418638165878213\n",
      "[201] loss: 0.10292934025898114\n",
      "[202] loss: 0.1027217172201931\n",
      "[203] loss: 0.10236505277871345\n",
      "[204] loss: 0.10296425533405743\n",
      "[205] loss: 0.10295713364337541\n",
      "[206] loss: 0.10222704530410144\n",
      "[207] loss: 0.1014281650137457\n",
      "[208] loss: 0.10186530481908262\n",
      "[209] loss: 0.09997903922376056\n",
      "[210] loss: 0.10202330254851291\n",
      "[211] loss: 0.10166460377459201\n",
      "[212] loss: 0.10046264404280586\n",
      "[213] loss: 0.09967321047547811\n",
      "[214] loss: 0.09910528381465014\n",
      "[215] loss: 0.10108705898110541\n",
      "[216] loss: 0.10049341585463989\n",
      "[217] loss: 0.09997940504264018\n",
      "[218] loss: 0.10079670868675161\n",
      "[219] loss: 0.0998979558624466\n",
      "[220] loss: 0.10129838719131043\n",
      "[221] loss: 0.09838970302841309\n",
      "[222] loss: 0.09764598298424519\n",
      "[223] loss: 0.09751042200726752\n",
      "[224] loss: 0.09769775366746121\n",
      "[225] loss: 0.10494359994526975\n",
      "[226] loss: 0.10060971975326538\n",
      "[227] loss: 0.09979778958588653\n",
      "[228] loss: 0.10158709187916717\n",
      "[229] loss: 0.09923327530254118\n",
      "[230] loss: 0.09717254950346783\n",
      "[231] loss: 0.09682677312528495\n",
      "[232] loss: 0.0984378822078431\n",
      "[233] loss: 0.09662509125350795\n",
      "[234] loss: 0.09725099426017415\n",
      "[235] loss: 0.09784885427642682\n",
      "[236] loss: 0.09963053187228138\n",
      "[237] loss: 0.09898688277426343\n",
      "[238] loss: 0.09679617421551151\n",
      "[239] loss: 0.09484067959537418\n",
      "[240] loss: 0.09566038387671391\n",
      "[241] loss: 0.0960313909982117\n",
      "[242] loss: 0.09696969254555539\n",
      "[243] loss: 0.09718445135477166\n",
      "[244] loss: 0.09731090636866063\n",
      "[245] loss: 0.09791733340724655\n",
      "[246] loss: 0.09797608816521879\n",
      "[247] loss: 0.09628986158865209\n",
      "[248] loss: 0.09561307692425962\n",
      "[249] loss: 0.09459981257500856\n",
      "[250] loss: 0.0938161570601952\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Train recurrent model\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "for epoch in range(250): \n",
    "    # TODO, implement early stopping with a validation holdout\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    dataset.reshuffle_sequence()\n",
    "    \n",
    "    for i, data in enumerate(dataloader):\n",
    "        hidden_out = (torch.rand(2 * 2, 24, 256, device=device),\n",
    "                      torch.rand(2 * 2, 24, 256, device=device))\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs = data[\"melody\"].squeeze_(dim=0).to(dtype=torch.float)\n",
    "        labels = data[\"harmony\"].squeeze_(dim=0).to(dtype=torch.float)\n",
    "        \n",
    "#         one_hot_expansion = []\n",
    "#         for sequence in labels:\n",
    "#             batch_expansion = []\n",
    "#             for index in sequence:\n",
    "#                 batch_expansion.append([1 if j == index else 0 for j in range(136)])\n",
    "#             one_hot_expansion.append(batch_expansion)\n",
    "            \n",
    "#         labels = torch.tensor(one_hot_expansion, dtype=torch.float, device=device)\n",
    "        \n",
    "#         print(inputs.shape)\n",
    "#         print(labels.shape)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs, hidden_out = net(inputs, hidden_out)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        hidden_out = (hidden_out[0].detach(), hidden_out[1].detach())\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "#     running_val_loss = 0\n",
    "#     for i in range(len(dataloader.validation_set)):\n",
    "#             data = dataloader.get_validation_set(i)\n",
    "#             inputs = data[\"melody\"].squeeze_().to(dtype=torch.float)\n",
    "#             labels = data[\"harmony\"].squeeze_().to(dtype=torch.float)\n",
    "            \n",
    "#             outputs, _ = net(inputs, hidden_out)\n",
    "#             val_loss = criterion(outputs, labels)\n",
    "            \n",
    "#             running_val_loss += loss.item()\n",
    "    \n",
    "    # print statistics    \n",
    "    print('[{}] loss: {}'.format(epoch + 1, running_loss / len(dataloader)))\n",
    "#     print('[{}] validation loss: {}'.format(epoch + 1, running_val_loss / len(dataloader.validation_set)))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (anaconda3 w pytorch)",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
