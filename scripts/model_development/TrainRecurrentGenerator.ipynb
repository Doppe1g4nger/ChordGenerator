{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelodyIndexToChordSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This dataset loads the training data linking a 64 * 1\n",
    "    melody index input to a 64 * 12 multi-hot chord sequence output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, transpose=None, strip_rest_edge_measures=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (string):\n",
    "                directory where .txt files of training data are stored\n",
    "            transpose (list[int]):\n",
    "                list of semitones to transpose the data by\n",
    "            strip_rest_edge_measures:\n",
    "                whether or not to skip measures before the first and after the last melody note\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_dir = data_dir\n",
    "        self.transpose = transpose\n",
    "        self.strip_rest_edge_measures = strip_rest_edge_measures\n",
    "        \n",
    "        self.melody_sequence = torch.Tensor()\n",
    "        self.harmony_sequence = torch.Tensor()\n",
    "        \n",
    "        for file in os.listdir(data_dir):\n",
    "            with open(os.path.join(data_dir, file)) as f_in:\n",
    "                file_text = f_in.read()\n",
    "                \n",
    "            lines = file_text.split(\"\\n\")\n",
    "            lines = [line for line in lines if line != \"\"]\n",
    "            \n",
    "            song_has_melody = False\n",
    "            for line in lines:\n",
    "                if line.split()[0] != \"12\":\n",
    "                    song_has_melody = True\n",
    "                    break\n",
    "                        \n",
    "            if song_has_melody:\n",
    "                if strip_rest_edge_measures:\n",
    "                    while lines[0].split()[0] == \"12\":\n",
    "                        lines.pop(0)\n",
    "\n",
    "                    while lines[-1].split()[0] == \"12\":\n",
    "                        lines.pop(-1)\n",
    "                    \n",
    "                song_melody = []\n",
    "                song_harmony = []\n",
    "\n",
    "                for line in lines:\n",
    "                    melody_index, harmony_vector = line.split()\n",
    "                    melody_index = int(melody_index)\n",
    "                    song_melody.append([melody_index])\n",
    "                    song_harmony.append([int(val) for val in harmony_vector])\n",
    "                    \n",
    "                song_melody = torch.tensor(song_melody, dtype=torch.float)\n",
    "                song_harmony = torch.tensor(song_harmony, dtype=torch.float)\n",
    "                \n",
    "                self.melody_sequence = torch.cat((self.melody_sequence, song_melody))\n",
    "                self.harmony_sequence = torch.cat((self.harmony_sequence, song_harmony))\n",
    "                    \n",
    "                if transpose:\n",
    "                    for semitone_amount in transpose:\n",
    "                        dot_index_bitmask = (song_melody != 12)\n",
    "                        shifted_melody = copy.deepcopy(song_melody)\n",
    "                        shifted_melody[dot_index_bitmask] = (song_melody[dot_index_bitmask] + semitone_amount) % 12\n",
    "                        shifted_harmony = copy.deepcopy(song_harmony).roll(semitone_amount, dims=1)\n",
    "\n",
    "                        self.melody_sequence = torch.cat((self.melody_sequence, shifted_melody))\n",
    "                        self.harmony_sequence = torch.cat((self.harmony_sequence, shifted_harmony))\n",
    "                    \n",
    "            else:\n",
    "                print(\"Skipping entry with no melody\")\n",
    "\n",
    "        self.melody_sequence = self.melody_sequence.view(1, -1)\n",
    "        self.harmony_sequence = self.harmony_sequence.view(1, -1)\n",
    "        \n",
    "        self.melody_sequence = self.melody_sequence.view(48, -1, 1)\n",
    "        self.harmony_sequence = self.harmony_sequence.view(48, -1, 12)\n",
    "\n",
    "    def __len__(self):\n",
    "        return ceil(self.melody_sequence.shape[1] / 64)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        melody, harmony = self.melody_sequence[:, idx*64:(idx + 1)*64], self.harmony_sequence[:, idx*64:(idx+1)*64]\n",
    "        \n",
    "        return {\"melody\": melody, \"harmony\": harmony}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input the directory where training files are stored: C:\\Users\\Danie\\PycharmProjects\\ChordGenerator\\data\\training_data\\training_dt_longnotes\n"
     ]
    }
   ],
   "source": [
    "training_data_fp = input(\"Input the directory where training files are stored: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input the name of the file you would like to store the model in: C:\\Users\\Danie\\PycharmProjects\\ChordGenerator\\data\\models\\model3.pt\n"
     ]
    }
   ],
   "source": [
    "model_path = input(\"Input the name of the file you would like to store the model in: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entry with no melody\n",
      "Skipping entry with no melody\n",
      "Skipping entry with no melody\n",
      "Skipping entry with no melody\n",
      "Skipping entry with no melody\n",
      "Skipping entry with no melody\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1122"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MelodyIndexToChordSequenceDataset(\n",
    "    training_data_fp,\n",
    "    transpose=[i for i in range(1, 12)],\n",
    "    strip_rest_edge_measures=True\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset)\n",
    "\n",
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.9330, 2.9330, 2.9330, 2.9330, 2.9330, 2.9330, 2.9330, 2.9330, 2.9330,\n",
      "        2.9330, 2.9330, 2.9330])\n"
     ]
    }
   ],
   "source": [
    "positive_examples = torch.zeros([12])\n",
    "\n",
    "for datum in dataloader:\n",
    "    data_batch = datum[\"harmony\"][0, ].to(torch.float).sum(1).sum(0)\n",
    "    positive_examples += data_batch\n",
    "\n",
    "negative_examples = (len(dataloader) * 48 * 64) - positive_examples\n",
    "\n",
    "positive_weights = torch.div(negative_examples, positive_examples).flatten()\n",
    "\n",
    "print(positive_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMGenerator(\n",
      "  (embedding): Embedding(13, 100)\n",
      "  (lstm): LSTM(100, 256, num_layers=2, batch_first=True)\n",
      "  (fc1): Linear(in_features=256, out_features=12, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define recurrent prediction model\n",
    "class LSTMGenerator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LSTMGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(13, 100)\n",
    "        self.lstm = nn.LSTM(input_size=100, hidden_size=256, num_layers=2, batch_first=True)\n",
    "        self.fc1 = nn.Linear(256, 12)\n",
    "        \n",
    "    def forward(self, x, hidden_in):\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(48, -1, 100)\n",
    "        x, h_out = self.lstm(x, hidden_in)\n",
    "        x = self.fc1(x)\n",
    "        return x, h_out\n",
    "\n",
    "\n",
    "net = LSTMGenerator()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 2.011\n",
      "[1,   100] loss: 1.976\n",
      "[1,   150] loss: 1.943\n",
      "[1,   200] loss: 1.914\n",
      "[1,   250] loss: 1.885\n",
      "[1,   300] loss: 1.852\n",
      "[1,   350] loss: 1.822\n",
      "[1,   400] loss: 1.799\n",
      "[1,   450] loss: 1.772\n",
      "[1,   500] loss: 1.745\n",
      "[1,   550] loss: 1.724\n",
      "[1,   600] loss: 1.709\n",
      "[1,   650] loss: 1.698\n",
      "[1,   700] loss: 1.689\n",
      "[1,   750] loss: 1.687\n",
      "[1,   800] loss: 1.686\n",
      "[1,   850] loss: 1.682\n",
      "[1,   900] loss: 1.674\n",
      "[1,   950] loss: 1.669\n",
      "[1,  1000] loss: 1.667\n",
      "[1,  1050] loss: 1.661\n",
      "[1,  1100] loss: 1.655\n",
      "[2,    50] loss: 1.654\n",
      "[2,   100] loss: 1.656\n",
      "[2,   150] loss: 1.658\n",
      "[2,   200] loss: 1.659\n",
      "[2,   250] loss: 1.660\n",
      "[2,   300] loss: 1.654\n",
      "[2,   350] loss: 1.652\n",
      "[2,   400] loss: 1.652\n",
      "[2,   450] loss: 1.651\n",
      "[2,   500] loss: 1.646\n",
      "[2,   550] loss: 1.645\n",
      "[2,   600] loss: 1.645\n",
      "[2,   650] loss: 1.646\n",
      "[2,   700] loss: 1.649\n",
      "[2,   750] loss: 1.652\n",
      "[2,   800] loss: 1.655\n",
      "[2,   850] loss: 1.655\n",
      "[2,   900] loss: 1.646\n",
      "[2,   950] loss: 1.643\n",
      "[2,  1000] loss: 1.642\n",
      "[2,  1050] loss: 1.637\n",
      "[2,  1100] loss: 1.630\n",
      "[3,    50] loss: 1.631\n",
      "[3,   100] loss: 1.632\n",
      "[3,   150] loss: 1.635\n",
      "[3,   200] loss: 1.635\n",
      "[3,   250] loss: 1.635\n",
      "[3,   300] loss: 1.630\n",
      "[3,   350] loss: 1.628\n",
      "[3,   400] loss: 1.627\n",
      "[3,   450] loss: 1.626\n",
      "[3,   500] loss: 1.621\n",
      "[3,   550] loss: 1.619\n",
      "[3,   600] loss: 1.619\n",
      "[3,   650] loss: 1.621\n",
      "[3,   700] loss: 1.623\n",
      "[3,   750] loss: 1.625\n",
      "[3,   800] loss: 1.626\n",
      "[3,   850] loss: 1.624\n",
      "[3,   900] loss: 1.613\n",
      "[3,   950] loss: 1.608\n",
      "[3,  1000] loss: 1.605\n",
      "[3,  1050] loss: 1.600\n",
      "[3,  1100] loss: 1.592\n",
      "[4,    50] loss: 1.592\n",
      "[4,   100] loss: 1.591\n",
      "[4,   150] loss: 1.593\n",
      "[4,   200] loss: 1.590\n",
      "[4,   250] loss: 1.589\n",
      "[4,   300] loss: 1.583\n",
      "[4,   350] loss: 1.582\n",
      "[4,   400] loss: 1.576\n",
      "[4,   450] loss: 1.575\n",
      "[4,   500] loss: 1.572\n",
      "[4,   550] loss: 1.569\n",
      "[4,   600] loss: 1.568\n",
      "[4,   650] loss: 1.571\n",
      "[4,   700] loss: 1.573\n",
      "[4,   750] loss: 1.573\n",
      "[4,   800] loss: 1.569\n",
      "[4,   850] loss: 1.563\n",
      "[4,   900] loss: 1.550\n",
      "[4,   950] loss: 1.542\n",
      "[4,  1000] loss: 1.536\n",
      "[4,  1050] loss: 1.534\n",
      "[4,  1100] loss: 1.527\n",
      "[5,    50] loss: 1.530\n",
      "[5,   100] loss: 1.524\n",
      "[5,   150] loss: 1.529\n",
      "[5,   200] loss: 1.524\n",
      "[5,   250] loss: 1.525\n",
      "[5,   300] loss: 1.520\n",
      "[5,   350] loss: 1.521\n",
      "[5,   400] loss: 1.513\n",
      "[5,   450] loss: 1.512\n",
      "[5,   500] loss: 1.515\n",
      "[5,   550] loss: 1.515\n",
      "[5,   600] loss: 1.515\n",
      "[5,   650] loss: 1.522\n",
      "[5,   700] loss: 1.526\n",
      "[5,   750] loss: 1.522\n",
      "[5,   800] loss: 1.514\n",
      "[5,   850] loss: 1.507\n",
      "[5,   900] loss: 1.499\n",
      "[5,   950] loss: 1.486\n",
      "[5,  1000] loss: 1.480\n",
      "[5,  1050] loss: 1.484\n",
      "[5,  1100] loss: 1.480\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Train recurrent model\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(weight=positive_weights)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(5):\n",
    "    \n",
    "    hidden_out = (torch.randn(2, 48, 256),\n",
    "                  torch.randn(2, 48, 256))\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs = data[\"melody\"].to(dtype=torch.long)[0, ]\n",
    "        labels = data[\"harmony\"].to(dtype=torch.float)[0, ]\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs, hidden_out = net(inputs, hidden_out)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        hidden_out = (hidden_out[0].detach(), hidden_out[1].detach())\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:    # print every 50 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 50))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
