{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import music21\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input a single part musicXML file to predict the chord progression for: C:\\Users\\Danie\\PycharmProjects\\ChordGenerator\\data\\test_data\\test_inputs\\Melody2.musicxml\n"
     ]
    }
   ],
   "source": [
    "file_to_predict = input(\"Input a single part musicXML file to predict the chord progression for: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input the file path of the model to use for chord prediction: C:\\Users\\Danie\\PycharmProjects\\ChordGenerator\\data\\models\\model2.pt\n"
     ]
    }
   ],
   "source": [
    "model_fp = input(\"Input the file path of the model to use for chord prediction: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input the name of the file to save the predicted multipart piece to: C:\\Users\\Danie\\PycharmProjects\\ChordGenerator\\data\\test_data\\test_outputs\\Melody2WithChords_m2.musicxml\n"
     ]
    }
   ],
   "source": [
    "prediction_file_name = input(\"Input the name of the file to save the predicted multipart piece to: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_per_measure = 16\n",
    "\n",
    "score = music21.converter.parse(file_to_predict)\n",
    "\n",
    "step_length = 4 / bins_per_measure\n",
    "\n",
    "melody = score.parts[0]\n",
    "\n",
    "melody_measures = melody.getElementsByClass(music21.stream.Measure)\n",
    "\n",
    "test_measures = []\n",
    "\n",
    "for i, mel_measure in enumerate(melody_measures):\n",
    "    test_measure = []\n",
    "    if mel_measure.timeSignature is not None:\n",
    "        step_length = (mel_measure.timeSignature.numerator / mel_measure.timeSignature.denominator\n",
    "                       * 4 / bins_per_measure)\n",
    "    melody_elements = [item for item in mel_measure.notesAndRests]\n",
    "    melody_index = 0\n",
    "\n",
    "    for i in range(bins_per_measure):\n",
    "        offset_timestep = i * step_length\n",
    "\n",
    "        if (melody_elements[melody_index] is not melody_elements[-1] \n",
    "            and melody_elements[melody_index + 1].offset <= offset_timestep):\n",
    "            melody_index += 1\n",
    "\n",
    "        melody_item = melody_elements[melody_index]\n",
    "        \n",
    "        item_index = melody_item.pitch.pitchClass if melody_item.name != \"rest\" else 12\n",
    "\n",
    "        test_measure.append([1 if j == item_index else 0 for j in range(13)])\n",
    "        \n",
    "    test_measures.append(test_measure)\n",
    "    \n",
    "test_data = []\n",
    "for i in range(len(test_measures)):\n",
    "                if i == 0:\n",
    "                    previous_melody_measure = [[0 for i in range(13)] for k in range(16)]\n",
    "                else:\n",
    "                    previous_melody_measure = test_measures[i-1]\n",
    "                    \n",
    "                if i == len(melody_measures) - 1:\n",
    "                    next_melody_measure = [[0 for i in range(13)] for k in range(16)]\n",
    "                else:\n",
    "                    next_melody_measure = test_measures[i+1]\n",
    "                    \n",
    "                melody_vector = previous_melody_measure + test_measures[i] + next_melody_measure\n",
    "                \n",
    "                melody_tensor = torch.tensor(melody_vector, dtype=torch.float)\n",
    "                \n",
    "                test_data.append(melody_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionedFeedforward(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ConditionedFeedforward, self).__init__()\n",
    "        self.fc1 = nn.Linear(48 * 13, 1024)\n",
    "        self.batch1 = nn.BatchNorm1d(1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.batch2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 16 * 12)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 48 * 13) # Flatten input for fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        x = self.batch1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batch2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "model = ConditionedFeedforward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConditionedFeedforward(\n",
       "  (fc1): Linear(in_features=624, out_features=1024, bias=True)\n",
       "  (batch1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (batch2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc3): Linear(in_features=512, out_features=192, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_fp))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "chord_measure_vectors = []\n",
    "\n",
    "for datum in test_data:\n",
    "    chord_measure_vectors.append(torch.sigmoid(model(datum)).view(16, 12) > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_part = music21.stream.Part()\n",
    "new_part.insert(0, music21.instrument.Piano())\n",
    "\n",
    "key = music21.key.Key(\"C\")\n",
    "time_sig = music21.meter.TimeSignature(\"4/4\")\n",
    "\n",
    "for chord_measure_vecs in chord_measure_vectors:\n",
    "    prior_vector = []\n",
    "    prior_chord = None\n",
    "    new_measure = music21.stream.Measure()\n",
    "    new_measure.insert(0, key)\n",
    "    new_measure.insert(0, time_sig)\n",
    "    for vector in chord_measure_vecs:\n",
    "        pitch_class_numbers = [i for i, val in enumerate(vector) if val == 1]\n",
    "        if pitch_class_numbers == prior_vector:\n",
    "            prior_chord.quarterLength += (4 / bins_per_measure)\n",
    "        else:\n",
    "            new_chord = music21.chord.Chord(pitch_class_numbers, quarterLength=(4 / bins_per_measure))\n",
    "            new_measure.append(new_chord)\n",
    "            prior_chord = new_chord\n",
    "        prior_vector = pitch_class_numbers\n",
    "        \n",
    "    new_part.append(new_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.insert(0, new_part)\n",
    "\n",
    "gex = music21.musicxml.m21ToXml.GeneralObjectExporter(score)\n",
    "    \n",
    "out = gex.parse()\n",
    "musicxml = out.decode('utf-8').strip()\n",
    "    \n",
    "with open(prediction_file_name, \"w\") as outfile:\n",
    "    outfile.write(musicxml)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
