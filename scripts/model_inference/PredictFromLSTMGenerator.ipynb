{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import music21\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_predict = input(\"Input a single part musicXML file to predict the chord progression for: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp = input(\"Input the file path of the model to use for chord prediction: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_file_name = input(\"Input the name of the file to save the predicted multipart piece to: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_per_measure = 16\n",
    "\n",
    "score = music21.converter.parse(file_to_predict)\n",
    "\n",
    "step_length = 4 / bins_per_measure\n",
    "\n",
    "melody = score.parts[0]\n",
    "\n",
    "melody_measures = melody.getElementsByClass(music21.stream.Measure)\n",
    "\n",
    "test_data = []\n",
    "for i, mel_measure in enumerate(melody_measures):\n",
    "    test_measure = []\n",
    "    if mel_measure.timeSignature is not None:\n",
    "        step_length = (mel_measure.timeSignature.numerator / mel_measure.timeSignature.denominator\n",
    "                       * 4 / bins_per_measure)\n",
    "    melody_elements = [item for item in mel_measure.notesAndRests]\n",
    "    melody_index = 0\n",
    "\n",
    "    for i in range(bins_per_measure):\n",
    "        offset_timestep = i * step_length\n",
    "\n",
    "        if (melody_elements[melody_index] is not melody_elements[-1] \n",
    "            and melody_elements[melody_index + 1].offset <= offset_timestep):\n",
    "            melody_index += 1\n",
    "\n",
    "        melody_item = melody_elements[melody_index]\n",
    "        \n",
    "        item_index = melody_item.pitch.pitchClass if melody_item.name != \"rest\" else 12\n",
    "\n",
    "        test_data.append([item_index])\n",
    "    \n",
    "melody_tensor = torch.tensor(test_data, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGenerator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LSTMGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(13, 100)\n",
    "        self.lstm = nn.LSTM(input_size=100, hidden_size=256, num_layers=2, batch_first=True)\n",
    "        self.fc1 = nn.Linear(256, 12)\n",
    "        \n",
    "    def forward(self, x, hidden_in):\n",
    "        x = self.embedding(x)\n",
    "        x, h_out = self.lstm(x, hidden_in)\n",
    "        x = self.fc1(x)\n",
    "        return x, h_out\n",
    "\n",
    "model = LSTMGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_fp))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_in = (torch.randn(2, 144, 256), torch.randn(2, 144, 256))\n",
    "\n",
    "chord_tensor = (torch.sigmoid(model(melody_tensor, hidden_in)[0]) > 0.45)\n",
    "\n",
    "chord_tensor = chord_tensor.view(-1, 12)\n",
    "print(chord_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_part = music21.stream.Part()\n",
    "new_part.insert(0, music21.instrument.Piano())\n",
    "\n",
    "time_sig = music21.meter.TimeSignature(\"4/4\")\n",
    "\n",
    "for measure in torch.chunk(chord_tensor, int(chord_tensor.shape[0] / 16)):\n",
    "    new_measure = music21.stream.Measure()\n",
    "    new_measure.insert(0, time_sig)\n",
    "    pitch_classes = [[i for i, val in enumerate(chord_vector) if val == 1] for chord_vector in measure]\n",
    "    pitch_class_to_quarterlength = [[pitch_classes[0], (4 / bins_per_measure)]]\n",
    "    for i, p_class in enumerate(pitch_classes[1:], 1):\n",
    "        if p_class == pitch_classes[i - 1]:\n",
    "            pitch_class_to_quarterlength[-1][1] += (4 / bins_per_measure)\n",
    "        else:\n",
    "            pitch_class_to_quarterlength.append([p_class, (4 / bins_per_measure)])\n",
    "            \n",
    "    for pitch_class_list, quarterlength in pitch_class_to_quarterlength:\n",
    "        if pitch_class_list != []:\n",
    "            new_measure.append(music21.chord.Chord(pitch_class_list, quarterLength=quarterlength))\n",
    "        else:\n",
    "            new_measure.append(music21.note.Rest(quarterLength=quarterlength))\n",
    "            \n",
    "    new_part.append(new_measure)\n",
    "            \n",
    "            \n",
    "new_part.show(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.insert(0, new_part)\n",
    "\n",
    "gex = music21.musicxml.m21ToXml.GeneralObjectExporter(score)\n",
    "    \n",
    "out = gex.parse()\n",
    "musicxml = out.decode('utf-8').strip()\n",
    "    \n",
    "with open(prediction_file_name, \"w\") as outfile:\n",
    "    outfile.write(musicxml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
